{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mllab.rl import TicTacToe, LGame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement value iteration and test it on two board games, Tic-tac-toe and the L-Game ([Wikipedia](https://en.wikipedia.org/wiki/L_game)).\n",
    "\n",
    "Both games have the same interface, so you can write your code to work for both games without further changes. The interface is shown in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = TicTacToe\n",
    "state = game.unique_states[2]  # A list of all possible game states. The states are normalized.\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game.unique_states[0]\n",
    "print(\"Current player:\", state.player)\n",
    "print(\"Winner?\", state.winner())  # returns either None or a player number\n",
    "print(\"Is terminal? \", state.is_terminal())  # is the game finished?\n",
    "print(\"List of valid actions: \", state.valid_actions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can apply an action and get a new state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "action = random.choice(state.valid_actions())\n",
    "print(\"Place piece at\", action)\n",
    "new_state = state.apply_action(action)\n",
    "new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized States\n",
    "\n",
    "The returned `new_state` is **not normalized**. Since the value function is only defined for normalized states. You have to normalize the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state_normalized = new_state.normalized()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "Implement the `win_reward` and `value_iteration` function below. Use the game interface explained above.\n",
    "\n",
    "**Pitfall**: Let $s$ be the current state, $a$ be a valid action for $s$, and $s^\\prime$ be the state we get if action $a$ is taken in state $s$. Then, the reward for the player taking action $a$ in $s$ is _not_ $V(s^\\prime)$, but $-V(s^\\prime)$. We use one value function for both policies and only store the value for normalized states (for equivalence classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "\n",
    "\n",
    "def win_reward(s, action=None):\n",
    "    \"\"\"\n",
    "    Compute the reward if in state s the given action is applied.\n",
    "    \n",
    "    If there is not winner, 0 is returned. Otherwise, 1 or -1 is returned,\n",
    "    depending on whether the current player has won or lost.\n",
    "    \"\"\"\n",
    "    # your code goes here\n",
    "\n",
    "\n",
    "def value_iteration(game, asynchronous=True, reward=win_reward):\n",
    "    \"\"\"\n",
    "    Perform value iteration and return the value function.\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    \n",
    "    game: A game class (e.g. TicTacToe or LGame)\n",
    "    asynchronous: bool\n",
    "        Whether to do the updates directly on the current iterate\n",
    "        or if the iterate is only updated at the end of the max operation (updated in parallel, synchronous).\n",
    "        With other words, if the parameter is False the maximum is computed\n",
    "        independently (using old values, ignoring other updates). Otherwise, other max operations\n",
    "        in the same iteration are taken into account.\n",
    "    reward: function\n",
    "        A function which takes a state and an action and returns a number.\n",
    "    \"\"\"\n",
    "    states = game.unique_states\n",
    "    v = {s: 0 for s in states}  # value function, initialized to 0\n",
    "    # your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute an optimal value function for the game Tic-Tac-Toe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vT_slow = value_iteration(TicTacToe, asynchronous=False)\n",
    "vT_fast = value_iteration(TicTacToe, asynchronous=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute an optimal value function for the L-Game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vL_slow = value_iteration(LGame, asynchronous=False)\n",
    "vL_fast = value_iteration(LGame, asynchronous=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code to get an policy from a value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class ValueFunctionBasedPolicy:\n",
    "    \"\"\"\n",
    "    A policy computed using a value function.\n",
    "    \n",
    "    Usage\n",
    "    =====\n",
    "    \n",
    "        # assume a value function is stored in v\n",
    "        policy = ValueFunctionBasedPolicy(v)\n",
    "\n",
    "        # get a best action for state\n",
    "        action = policy[state]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, v, reward):\n",
    "        self._v = v\n",
    "        self._reward = reward\n",
    "    \n",
    "    def __getitem__(self, s):\n",
    "        \"\"\"Get an action for state s.\"\"\"\n",
    "        return random.choice(self.actions(s))\n",
    "    \n",
    "    def actions(self, s):\n",
    "        \"\"\"Get all actions which maximize the reward in state s.\"\"\"\n",
    "        actions = list(s.valid_actions())\n",
    "        if not actions:\n",
    "            return []\n",
    "        # your code goes here\n",
    "\n",
    "    def value(self, s, a):\n",
    "        return self._reward(s, a) - self._v[s.apply_action(a).normalized()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us watch the agent play against itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mllab.rl import self_play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_play(TicTacToe, ValueFunctionBasedPolicy(vT_fast, win_reward), sleep=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_play(LGame, ValueFunctionBasedPolicy(vL_fast, win_reward), sleep=0.01, max_steps=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L-Game Insights and Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L-Game does never finish if both players are perfect. There some states though, which are special. Let us use the value function to get some insights.\n",
    "\n",
    "### Task 2\n",
    "\n",
    "Collect all states with negative value. How many are terminal, how many are not terminal? What does it mean that a state has a negative value but is not terminal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the agent start in every state with negative reward which is not terminal and let it play against itself. Use\n",
    "```python\n",
    "final_state, steps = self_play(state, ValueFunctionBasedPolicy(vL_fast), max_steps=100, sleep=0)\n",
    "```\n",
    "to compute the steps until the game terminates and it final state.\n",
    "\n",
    "Print the number of steps it takes to terminate and how the game ended. Run the code several times, what do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the computed value function to design a new reward which improves the behaivor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_positions = set(s for s in LGame.unique_states if vL[s] > 0 and s.winner() is None)\n",
    "bad_positions = set(s for s in LGame.unique_states if vL[s] < 0 and s.winner() is None)\n",
    "\n",
    "def asap_win_reward(s, a=None):\n",
    "    # your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vL_improved = value_iteration(LGame, reward=asap_win_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compute the number of steps before termination again and compare. Maybe, also read the Wikipedia article on the L-Game referenced at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Implement `policy_evaluation` and `policy_improvement` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from mllab.rl import Policy\n",
    "from itertools import cycle\n",
    "\n",
    "\n",
    "def policy_improvement(p1, p2, v1, v2, reward):\n",
    "    \"\"\"\n",
    "    Compute an improved policy for the states using the value function v.\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    p1: dict\n",
    "        Policy for player 1\n",
    "    p2: dict\n",
    "        Policy for player 2\n",
    "    v1: dict\n",
    "        Value function for player 1\n",
    "    v2: dict\n",
    "        Value function for player 2\n",
    "    reward: callable\n",
    "        The reward function\n",
    "    Returns\n",
    "    =======\n",
    "    (dict, dict)\n",
    "        Two policies, one for player 1, the second for player 2\n",
    "    \"\"\"\n",
    "    p1 = dict(p1)\n",
    "    states = list(v1.keys())\n",
    "    # compute greedy update for p1\n",
    "    for s in states:\n",
    "        # your code goes here\n",
    "    # compute best response to p1 for p2\n",
    "    v1 = policy_evaluation(states, p1, p2, reward)\n",
    "    v2 = policy_evaluation(states, p2, p1, reward)\n",
    "    p2 = {}\n",
    "    while True:\n",
    "        print('P', end='')  # print a 'P' for each iteration\n",
    "        # your code goes here\n",
    "    return p1, p2\n",
    "\n",
    "\n",
    "def policy_evaluation(states, p1, p2, reward):\n",
    "    \"\"\"\n",
    "    Comptue the value function for given policies.\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    states: list of normalized states\n",
    "    p1: dict\n",
    "        Policy for first player\n",
    "    p2: dict\n",
    "        Policy for second player\n",
    "    \"\"\"\n",
    "    v = {}\n",
    "    for s in states:\n",
    "        current = s\n",
    "        rewards = []\n",
    "        sign = 1  # cf. pitfall\n",
    "        # store (sign, state) pairs and check for cycles!\n",
    "        trajectory = []\n",
    "        # iterate until the current state is terminal (i.e., None)\n",
    "        while current is not None:\n",
    "            # your code goes here\n",
    "        v[s] = sum(rewards)\n",
    "    return v\n",
    "\n",
    "\n",
    "def policy_iteration(game, reward):\n",
    "    \"\"\"\n",
    "    Perform policy iteration for the given game, and reward function.\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    policy: Policy\n",
    "    \"\"\"\n",
    "    states = game.unique_states\n",
    "    p1 = {s: random.choice(s.valid_actions()) for s in states if s.valid_actions()}\n",
    "    p2 = {s: random.choice(s.valid_actions()) for s in states if s.valid_actions()}\n",
    "    v1, v2 = {}, {}\n",
    "    while True:\n",
    "        v1_new = policy_evaluation(states, p1, p2, reward)\n",
    "        v2_new = policy_evaluation(states, p2, p1, reward)\n",
    "        if v1 == v1_new and v2 == v2_new:\n",
    "            break\n",
    "        v1, v2 = v1_new, v2_new\n",
    "        p1, p2 = policy_improvement(p1, p2, v1, v2, reward)\n",
    "        print('.', end='')\n",
    "    print('')\n",
    "    return Policy(game, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pT_policy_iteration = policy_iteration(TicTacToe, win_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The policy iteration does might not converge for the L-Game, more specifically, finding the best response in `policy_improvement`. What it the reason? Can you fix it? (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pL_pi = policy_iteration(LGame, asap_win_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Learning (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install some Python package we need by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym box2d box2d-kengz opencv-python h5py tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from mllab.rl.dqn import BaseQNetwork, ReplayMemory, EpsilonGreedyPolicy, ProportionalPrioritizationReplayMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "\n",
    "The action space of the car racing environment is continous and\n",
    "consists of a three dimensional real vector $[-1, 1]x[0, 1]x[0, 1]$\n",
    "corresponding to steering position, amount of gas and and brake intensity.\n",
    "We need discrete actions, so you have to pick finitely many points from this box.\n",
    "\n",
    "A initial suggestion has been made, **feel free to modify it**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-mllab-v0', verbose=0)\n",
    "# This will open a window. Call env.close() at the end to get rid of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picke a finite set of actions\n",
    "action_space = env.action_space.discretize((\n",
    "    np.array([ 0, 1, 0]),  # full gas\n",
    "    np.array([ 0, 0, 1]),  # full brake\n",
    "    np.array([-1, 0, 0]),  # steer left\n",
    "    np.array([ 1, 0, 0]),  # steer right\n",
    "    np.array([ 0, 0, 0]),  # do nothing\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "action_space = env.action_space\n",
    "print(action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's watch a random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "start = time.process_time()\n",
    "current = time.process_time()\n",
    "\n",
    "env.reset()\n",
    "while  True:\n",
    "    current = time.process_time()\n",
    "    new_state, reward, terminated, _info = env.step(action_space.sample())\n",
    "    env.render()\n",
    "    if (current - start) >= 1:\n",
    "        break\n",
    "print(new_state)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing map\n",
    "\n",
    "The state space of the car racing environment is made of an $96\\times96$ RGB image and seven measurements:\n",
    "\n",
    "- The velocity of the car (absolute value)\n",
    "- The angular velocity of the four wheels\n",
    "- The steering angle of the front wheels\n",
    "- The angular velocity of the car\n",
    "\n",
    "The map `preprocess` takes a state and transforms it to a state which hopefully is better suited as an input to the neural network. You can use the transformation as is **or change it**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def show(image):\n",
    "    \"\"\"\n",
    "    Show a greyscale image.\n",
    "    \n",
    "    Useful for debugging.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(dpi=2 * 72)\n",
    "    if image.ndim == 3:\n",
    "        if image.shape[0] == 1:\n",
    "            image = image.reshape(image.shape[1:])\n",
    "        elif image.shape[-1] == 1:\n",
    "            image = image.reshape(image.shape[:-1])\n",
    "    if image.ndim == 2:\n",
    "        ax.imshow(image, cmap='gray')\n",
    "    else:\n",
    "        ax.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def preprocess(state):\n",
    "    \"\"\"\n",
    "    Preprocess the rendered color image of the car racing environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    state: (image, measurements)\n",
    "        image is an RGB image, more precisely an 96x96x3 array.\n",
    "        measurements is 1D vector of length 7.\n",
    "    \"\"\"\n",
    "    image, measurements = state\n",
    "    # Convert to grayscale\n",
    "    gray = cv.cvtColor(image, cv.COLOR_RGB2GRAY)\n",
    "    # Resize the image (to save memory)\n",
    "    # Get mask for red markings in curves\n",
    "    curve_marks = cv.inRange(image, (250, 0, 0), (255, 0, 0))\n",
    "    # Replace markings with white\n",
    "    gray[curve_marks == 255] = 255\n",
    "    gray = cv.resize(gray, (0,0), fx=0.85, fy=0.85)\n",
    "    # Remove pattern in grass by setting light pixels (> 130) to white (255)\n",
    "    gray = cv.threshold(gray, 130, 255, cv.THRESH_TRUNC)[1] / 130\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        gray = gray.reshape((1,) + gray.shape)\n",
    "    else:\n",
    "        gray = gray.reshape(gray.shape + (1,))\n",
    "    measurements = np.concatenate((\n",
    "        measurements[:4],\n",
    "        np.array([np.cos(measurements[4]), np.sin(measurements[4])]),\n",
    "        measurements[5:],\n",
    "    ))\n",
    "    return (gray.astype(K.floatx()), measurements.astype(K.floatx()))\n",
    "\n",
    "def preprocess_mount(state):\n",
    "    return (state[0],state[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Network\n",
    "\n",
    "The Q-Network maps a (preprocessd) state to a Q-value for each action. Since out state consists of an image and measurements, we need to use Keras' functional API to build a neural network which can take mixed input.\n",
    "\n",
    "First, define two models for the scalar inputs and the image inputs:\n",
    "\n",
    "```python\n",
    "input_img = layers.Input(shape=...)\n",
    "img = layers.Conv2D(4, kernel_size=(3, 3), activation='relu')(input_img)\n",
    "# add more layers here (replace input_img by img)\n",
    "img = layers.Flatten()(img)\n",
    "img = keras.Model(input_img, img)\n",
    "\n",
    "input_scalar = layers.Input(shape=...)\n",
    "img = keras.Dense(8, activation='relu')(input_scalar)\n",
    "# as above\n",
    "scalar = layers.Model(input_scalar, scalar)\n",
    "```\n",
    "\n",
    "Then concatenate both models and create a new model:\n",
    "```python\n",
    "model = layers.concatenate([img.output, scalar.output])\n",
    "model = layers.Dense(num_actions, activation='linear')(model)\n",
    "model = keras.Model(inputs=[img.input, scalar.input], outputs=model)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "Define your model for the Q-network by implementing the `build_model` method.\n",
    "\n",
    "The method must return a model and an optimizer. The loss is implemented in the parent class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.layers as layers\n",
    "import keras.optimizers as optimizers\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "class QNetworkCar(BaseQNetwork):\n",
    "    model_type = 'car'\n",
    "    \n",
    "    def build_model(self, state_shape):\n",
    "        num_actions = len(self.action_space)\n",
    "        # Build the network for the image part\n",
    "        img_shape, scalar_shape = state_shape\n",
    "\n",
    "        input_img = layers.Input(shape=img_shape)\n",
    "\n",
    "        img = layers.Conv2D(4, kernel_size=(3, 3), activation='relu')(input_img)\n",
    "        img = layers.Conv2D(4, kernel_size=(3, 3), activation='relu')(img)\n",
    "        # add more layers here (replace input_img by img)\n",
    "        img = layers.Flatten()(img)\n",
    "        img = keras.Model(input_img, img)\n",
    "\n",
    "        # Build the network for the scalar part\n",
    "        input_scalar = layers.Input(shape=scalar_shape)\n",
    "        scalar = layers.Dense(8, activation='relu')(input_scalar)\n",
    "        scalar = keras.Model(input_scalar, scalar)\n",
    "        # Combine both networks\n",
    "        model = layers.concatenate([img.output, scalar.output])\n",
    "        # add your layers, if any, here\n",
    "        # the output shape must be the number of actions!\n",
    "        model = layers.Dense(num_actions, activation='linear')(model)\n",
    "        model = keras.Model(inputs=[img.input, scalar.input], outputs=model)\n",
    "        \n",
    "\n",
    "        opt = optimizers.RMSprop(lr=0.00025 / 4, rho=0.95, epsilon=0.01)\n",
    "\n",
    "        return model, opt\n",
    "    \n",
    "class QNetworkMount(BaseQNetwork):\n",
    "    model_type = 'mountain'\n",
    "    \n",
    "    def build_model(self,state_shape):\n",
    "        \n",
    "        num_actions = 3\n",
    "        \n",
    "        # This returns a tensor\n",
    "        inputs = Input(shape=(num_actions,))\n",
    "        # a layer instance is callable on a tensor, and returns a tensor\n",
    "        x = Dense(3, activation='relu')(inputs)\n",
    "        predictions = Dense(1, activation='linear')(x)\n",
    "\n",
    "        # This creates a model that includes\n",
    "        # the Input layer and three Dense layers\n",
    "        \n",
    "        model = Model(inputs=[inputs,], outputs=predictions)\n",
    "\n",
    "        opt = optimizers.RMSprop(lr=0.00025 / 4, rho=0.95, epsilon=0.01)\n",
    "\n",
    "        return model, opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory\n",
    "\n",
    "The replay memory stores transitions. It was already implemented for your. To add a transition use\n",
    "```python\n",
    "replay_memory.add(state, action_index, reward, new_state)\n",
    "```\n",
    "**Important:** `state` and `new_state` must be the output of `preprocess`. If `state` is terminal, `new_state` must be `None`. The action index (not the actual index) is returned by the policy, see below.\n",
    "\n",
    "\n",
    "In order to sample a batch of transitions, call\n",
    "```python\n",
    "transitions, sample_weights = replay_memory.sample(importance_criterion, progress)\n",
    "```\n",
    "The parameter `importance_criterion` is a callable (e.g., a function) which get a transitions as arguments and returns an number to measure the prediction error for the transitions. You should use the TD-Error\n",
    "$$\n",
    "    |y - Q(s^\\prime, a)| = |\\bigl(r + \\gamma Q_\\textrm{target}(s^\\prime, \\operatorname{argmax}_aQ(s^\\prime, a))\\bigr) - Q(s^\\prime)|.\n",
    "$$\n",
    "For terminal states $y$ is just $r$.\n",
    "\n",
    "The arguments for `importance_criterion` are\n",
    "```python\n",
    "def my_criterion(s, actions, rewards, s2, not_terminal): ...\n",
    "```\n",
    "Where\n",
    "- `s` is a list of preprocessed state\n",
    "- `actions` is a NumPy array of action indices (the action taken in `s`)\n",
    "- `rewards` is a NumPy array of rewards received (the reward received after taking the action from `actions` in the state from `s`)\n",
    "- `s2` is a list of preprocessd states (the new state). Only non terminal states are returned.\n",
    "- `not_terminal` is a NumPy array of boolean indicating which of the states in `s` was not terminal. \n",
    "\n",
    "The parameter `progression` is a float in $[0, 1]$ which represents the percentage of the steps taken so far.\n",
    "\n",
    "The return value `transitions` of `replay_memory.sample` is a tuple which has the same entries as those given as parameters to `importance_criterion`. The `sample_weights` return value must be passed to the gradient step (see policy description).\n",
    "\n",
    "#### Memory requirements\n",
    "Depending on the size of your state the memory requirements can be huge. For example, to store 100k transitions you need 10GB of memory or more. Check if your machine has enough memory or try a smaller replay memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\varepsilon$-Greedy-Policy\n",
    "\n",
    "A policy class is already implemented for your. Initialize it as following (feel free to change the parameters):\n",
    "```python\n",
    "policy = EpsilonGreedyPolicy(q_network)\n",
    "policy.initial_exploration = 1.0  # initial epsilon value\n",
    "policy.final_exploration = 0.01  # lowest epsilon value\n",
    "policy.evaluation_exploration = 0.001  # epsilon used during evaluation\n",
    "policy.final_exploration_step = 500_000  # number of steps over which epsilon is linearly decreased\n",
    "```\n",
    "Here, `q_network` is an instance of `QNetwork`.\n",
    "\n",
    "With probability $\\varepsilon$ the policy returns a random action (exploration). Otherwise, an action is returned with maximal Q-value. The probability $\\varepsilon$ is linearly decreased with the step number.\n",
    "\n",
    "You get an action from the policy by calling it (like a function):\n",
    "```python\n",
    "action_index, action = policy(preprocessed_state, step)\n",
    "```\n",
    "\n",
    "More methods:\n",
    "\n",
    "- `policy.copy()` creates an independent copy of the policy\n",
    "- `policy.gradient_step(states, actions, labels, sample_weights)` performs a gradient step. `state` and `actions` are the return values of the replay memory (first two elements in `transitions`), and `sample_weights` is the second return value of the replay memory.\n",
    "- `policy.copy_weights_from(other_policy)` Copies over the weights from another policy.\n",
    "\n",
    "To compute the Q-values of the underlying network, use\n",
    "```python\n",
    "policy.q_network(states)\n",
    "```\n",
    "which returns a NumPy array where each row contains the outputs of the network. You need this to implement the label computation and for `importance_criterion`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning Algorithm\n",
    "\n",
    "Implement the `train` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "class DeepQLearning:\n",
    "    # After how many steps the weights are copied to the target-action network\n",
    "    target_network_update_frequency = 1_000\n",
    "    discount_factor = 0.99\n",
    "    # A random policy is run for that many steps to initialize the replay memory\n",
    "    replay_start_size = 5_000\n",
    "\n",
    "    def __init__(self, env, replay_memory, policy, preprocess=preprocess):\n",
    "        self.env = env\n",
    "        self.replay_memory = replay_memory\n",
    "        self.policy = policy\n",
    "        self.preprocess = preprocess\n",
    "        self.rewards = []\n",
    "        self.best_agent = None\n",
    "\n",
    "\n",
    "    def train(self, total_steps, replay_period, weight_filename=None, evaluate=None, double_dqn=False):\n",
    "        \"\"\"\n",
    "        Train the agent using DQN.\n",
    "\n",
    "        Parameters\n",
    "        ==========\n",
    "\n",
    "        total_steps: int\n",
    "            Number of steps the agent is trained for.\n",
    "        replay_period: int\n",
    "            Number of steps between which the network is trained.\n",
    "        weight_filename: str or None\n",
    "            If not None the weights of Q-network are stored to this file during training.\n",
    "        evaluate: int or None\n",
    "            Number of episodes after which the policy is evaluted and the result is printed.\n",
    "        double_qdn: bool\n",
    "            Whether to use Double-DQN (DDQN).\n",
    "        \"\"\"\n",
    "        if len(self.replay_memory) == 0:\n",
    "            self.initialize_replay_memory()\n",
    "        self.action_value = self.policy\n",
    "        self.target_action_value = self.policy\n",
    "        episode = 0\n",
    "        step = 0\n",
    "\n",
    "        while episode < total_steps:\n",
    "            print(episode/total_steps)\n",
    "            episode += 1\n",
    "            self.env.reset()\n",
    "            preprocessed_state = self.preprocess(self.env.state)\n",
    "            print(\"Episode {} ({} steps so far)\".format(episode, step))\n",
    "            \n",
    "            \n",
    "            #for _ in tqdm(range(env.spec.max_episode_steps)):\n",
    "            step=0\n",
    "            for _ in tqdm(range(100)):\n",
    "                step += 1\n",
    "                preprocessed_state = self.preprocess(self.env.state)\n",
    "                \n",
    "                action_index, action = policy(preprocessed_state, step)\n",
    "               \n",
    "                new_state, reward, *_ = env.step(action)\n",
    "                new_state = self.preprocess(new_state)\n",
    "                replay_memory.add(preprocessed_state, action_index, reward, new_state)\n",
    "                \n",
    "                if step % replay_period == 0:\n",
    "                    \n",
    "                    transitions, sample_weights = self.replay_memory.sample(self.importance_criterion, progression = step/total_steps)\n",
    "                    nt= transitions[4]\n",
    "                    y = transitions[2]\n",
    "                    s2= transitions[3]\n",
    "                    y[nt] = y[nt] + self.discount_factor*self.target_action_value.q_network(s2)[np.arange(s2[0].shape[0]), np.argmax(self.action_value.q_network(s2),axis=1)]\n",
    "                    \n",
    "                    self.target_action_value.gradient_step(*transitions[0:2], y, sample_weights)\n",
    "                    \n",
    "                if step % self.target_network_update_frequency ==0:\n",
    "                    self.action_value.copy_weights_from(self.target_action_value)\n",
    "                    \n",
    "            if evaluate is not None and episode % evaluate == 0:\n",
    "                total_reward = self.evaluate(self.target_action_value, weight_filename)\n",
    "                print(\"Total reward: {}\".format(total_reward))\n",
    "                \n",
    "\n",
    "    def initialize_replay_memory(self):\n",
    "        \"\"\"Initialize the replay memory using a random policy.\"\"\"\n",
    "        self.env.reset()\n",
    "        self.replay_memory.purge()\n",
    "        state = self.preprocess(self.env.state)\n",
    "        size = min(self.replay_start_size, self.replay_memory.capacity)\n",
    "        print(\"Initialize replay memory with {} transitions\".format(size))\n",
    "        for _ in tqdm(range(size)):\n",
    "            if self.policy.q_network.model_type == 'car':\n",
    "                action_index, action = self.policy.sample(return_index=True)\n",
    "            elif self.policy.q_network.model_type == 'mountain':\n",
    "                action_index, action = self.policy.sample(),self.policy.sample()\n",
    "                \n",
    "            new_state, reward, terminated, _info = self.env.step(action)\n",
    "            new_state = self.preprocess(new_state)\n",
    "            self.replay_memory.add(state, action_index, reward, new_state)\n",
    "            if terminated:\n",
    "                self.env.reset()\n",
    "                state = self.preprocess(self.env.state)\n",
    "            else:\n",
    "                state = new_state\n",
    "\n",
    "    def evaluate(self, policy, weight_filename=None):\n",
    "        state = self.env.reset()\n",
    "        total_reward = 0\n",
    "        for _ in tqdm(range(env.spec.max_episode_steps)):\n",
    "            # get action from policy\n",
    "            _, action = policy(self.preprocess(state))\n",
    "            state, r, terminal, _ = self.env.step(action)\n",
    "            total_reward = r + total_reward\n",
    "            if terminal:\n",
    "                break\n",
    "        if self.best_agent is None or total_reward > max(self.rewards):\n",
    "            self.best_agent = policy.copy()\n",
    "            if weight_filename is not None:\n",
    "                self.best_agent.q_network.save(weight_filename + '.best')\n",
    "        self.rewards.append(total_reward)\n",
    "        return total_reward\n",
    "    \n",
    "    def importance_criterion(self, s, actions, rewards, s2, nt):\n",
    "        y = rewards\n",
    "        y[nt] = y[nt] + self.discount_factor*self.target_action_value.q_network(s2)[np.arange(s2[0].shape[0]), np.argmax(self.action_value.q_network(s2),axis=1)]\n",
    "        w = np.max(self.action_value.q_network(s2),axis=1)\n",
    "        return np.abs(y-w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create all objects, set parameters, and start training. **Make sure the replay memory is not too big for your memory!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountain Car ( Not Working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import Dense \n",
    "\n",
    "# ---- Mountain Car -----\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "action_space = env.action_space\n",
    "\n",
    "q_network = QNetworkMount((2,), action_space)\n",
    "\n",
    "policy = EpsilonGreedyPolicy(q_network)\n",
    "policy.initial_exploration = 1.0  # initial epsilon value\n",
    "policy.final_exploration = 0.01  # lowest epsilon value\n",
    "policy.evaluation_exploration = 0.001  # epsilon used during evaluation\n",
    "policy.final_exploration_step = 500  # number of steps over which epsilon is linearly decreased\n",
    "\n",
    "# Create the (empty) replay memory\n",
    "replay_memory = ProportionalPrioritizationReplayMemory(\n",
    "    (1,),\n",
    "    # ATTENTION: This is most likely too much for a laptop\n",
    "    capacity=500, batch_size=32)\n",
    "\n",
    "dqn = DeepQLearning(env, replay_memory, policy,preprocess = preprocess_mount)\n",
    "dqn.target_network_update_frequency = 50 #5000\n",
    "dqn.replay_start_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "() ()\n",
      "Initialize replay memory with 64 transitions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6213911872d04713a55bf6dfa2a2969a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=64), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "0.0\n",
      "() ()\n",
      "Episode 1 (0 steps so far)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1597959bab334563a073fb4cdbb90395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "() ()\n",
      "1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[-0.469495  ],\n       [-0.47360775],\n       [-0.49301416],\n       [-0.51150113],\n       [-0.52384526],\n       [-0.53044635],\n       [-0.5341803 ],\n       [-0.54677486],\n       [-0.561777  ],\n ...",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-499212ed152a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplay_period\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_filename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"agentM.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-50-5b9675e911a2>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, total_steps, replay_period, weight_filename, evaluate, double_dqn)\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mreplay_period\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                     \u001b[0mtransitions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimportance_criterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogression\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtotal_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m                     \u001b[0mnt\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\Machine Learning Course\\Machine-Learning-Practical-Lab\\mllab\\rl\\dqn.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, importance_criterion, progression, batch_size)\u001b[0m\n\u001b[0;32m    270\u001b[0m         )\n\u001b[0;32m    271\u001b[0m         \u001b[1;31m# update transition priorities\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 272\u001b[1;33m         \u001b[0mimportance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportance_criterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    273\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m             \u001b[0mimportance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-5b9675e911a2>\u001b[0m in \u001b[0;36mimportance_criterion\u001b[1;34m(self, s, actions, rewards, s2, nt)\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscount_factor\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_action_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\Machine Learning Course\\Machine-Learning-Practical-Lab\\mllab\\rl\\dqn.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, states)\u001b[0m\n\u001b[0;32m    367\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 369\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\janwu\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1147\u001b[0m                              'argument.')\n\u001b[0;32m   1148\u001b[0m         \u001b[1;31m# Validate user data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1149\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1150\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\janwu\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\janwu\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[1;34m'Expected to see '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' array(s), '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m                 \u001b[1;34m'but instead got the following list of '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             raise ValueError(\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[-0.469495  ],\n       [-0.47360775],\n       [-0.49301416],\n       [-0.51150113],\n       [-0.52384526],\n       [-0.53044635],\n       [-0.5341803 ],\n       [-0.54677486],\n       [-0.561777  ],\n ..."
     ]
    }
   ],
   "source": [
    "dqn.train(total_steps=1, replay_period=10, evaluate=1, weight_filename=\"agentM.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Racing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Car Racing ---0\n",
    "\n",
    "env = gym.make('CarRacing-mllab-v0', verbose=0)\n",
    "\n",
    "action_space = env.action_space.discretize((\n",
    "    np.array([ 0, 1, 0]),  # full gas\n",
    "    np.array([ 0, 0, 1]),  # full brake\n",
    "    np.array([-1, 0, 0]),  # steer left\n",
    "    np.array([ 1, 0, 0]),  # steer right\n",
    "    np.array([ 0, 0, 0]),  # do nothing\n",
    "))\n",
    "\n",
    "# Get shape of transformed state\n",
    "s = preprocess(env.reset())\n",
    "img_shape = s[0].shape\n",
    "scalar_shape = s[1].shape\n",
    "\n",
    "# Create the Q-Network\n",
    "q_network = QNetworkCar((img_shape, scalar_shape), action_space)\n",
    "\n",
    "policy = EpsilonGreedyPolicy(q_network)\n",
    "policy.initial_exploration = 1.0  # initial epsilon value\n",
    "policy.final_exploration = 0.01  # lowest epsilon value\n",
    "policy.evaluation_exploration = 0.001  # epsilon used during evaluation\n",
    "policy.final_exploration_step = 500  # number of steps over which epsilon is linearly decreased\n",
    "\n",
    "# Create the (empty) replay memory\n",
    "replay_memory = ProportionalPrioritizationReplayMemory(\n",
    "    img_shape, scalar_shape,\n",
    "    # ATTENTION: This is most likely too much for a laptop\n",
    "    capacity=500, batch_size=32)\n",
    "\n",
    "dqn = DeepQLearning(env, replay_memory, policy)\n",
    "dqn.target_network_update_frequency = 50 #5000\n",
    "dqn.replay_start_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize replay memory with 64 transitions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e524fb1f20346cba26133023a008df6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=64), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "Episode 1 (0 steps so far)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97875583a8a4201a4bfd0f0c3f962e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652bdead693b49cebec47f5e984ad6f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: -77.01149425287345\n"
     ]
    }
   ],
   "source": [
    "#dqn.train(episodes=10, max_steps_per_episode=1000, evaluate=5, weight_filename=\"agent.h5\")\n",
    "dqn.train(total_steps=1, replay_period=10, evaluate=1, weight_filename=\"agent.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googledrivedownloader\n",
      "  Downloading https://files.pythonhosted.org/packages/3a/5c/485e8724383b482cc6c739f3359991b8a93fb9316637af0ac954729545c9/googledrivedownloader-0.4-py2.py3-none-any.whl\n",
      "Installing collected packages: googledrivedownloader\n",
      "Successfully installed googledrivedownloader-0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install googledrivedownloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdd.download_file_from_google_drive(file_id='1iytA1n2z4go3uVCwE__vIKouTKyIDjEq',\n",
    "                                    dest_path='./data/mnist.zip',\n",
    "                                    unzip=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.policy.q_network.load(\"agent.h5.best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can watch the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def render_policy(env, preprocess, policy):\n",
    "    \"\"\"Visualize a policy for an environment.\"\"\"\n",
    "\n",
    "    start = time.process_time()\n",
    "    current = time.process_time()\n",
    "    env.reset()\n",
    "\n",
    "\n",
    "    while True:\n",
    "        current = time.process_time()\n",
    "        state = preprocess(env.state)\n",
    "        terminal = env.step(policy(state)[1])[2]\n",
    "        env.render()\n",
    "        if (current - start) >= 30:\n",
    "            break\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: To record the video uncomment the following lines \n",
    "# and change \"env\" in the call to render_policy below to \"rec_env\"\n",
    "\n",
    "# rec_env = gym.wrappers.Monitor(env, \"recording\", video_callable=lambda episode_id: True, force=True)\n",
    "# rec_env.reset_video_recorder()\n",
    "render_policy(env, preprocess, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
